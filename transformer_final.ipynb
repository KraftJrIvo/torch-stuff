{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031e6bd9-75e4-463d-9087-3954544acc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"0\"\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as NN\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('data/messages.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))\n",
    "i2c = { i:ch for i, ch in enumerate(chars) }\n",
    "c2i = { ch[1]:i for i, ch in enumerate(i2c.items())}\n",
    "N = len(chars)\n",
    "LEN = len(text)\n",
    "encode = lambda s: [c2i[c] for c in s]\n",
    "decode = lambda l: ''.join([i2c[i] for i in l])\n",
    "text_enc = encode(text)\n",
    "\n",
    "CTX_SZ = 256 #8\n",
    "xs, ys = [], []\n",
    "for s in range(LEN - CTX_SZ):\n",
    "    xs.append(text_enc[s:s+CTX_SZ])\n",
    "    ys.append(text_enc[s+1:s+CTX_SZ+1])\n",
    "\n",
    "tmp = list(zip(xs, ys))\n",
    "random.shuffle(tmp)\n",
    "xs, ys = zip(*tmp)\n",
    "xs, ys = list(xs), list(ys)\n",
    "\n",
    "n = int(0.9 * LEN)\n",
    "xs_trn, ys_trn = torch.tensor(xs[:n]), torch.tensor(ys[:n])\n",
    "xs_val, ys_val = torch.tensor(xs[n:]), torch.tensor(ys[n:])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "xs_trn, ys_trn = xs_trn.to(device), ys_trn.to(device)\n",
    "xs_val, ys_val = xs_val.to(device), ys_val.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5006050d-fe2c-4c60-a08e-e926d46ef602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(NN.Module):\n",
    "    def __init__(self, head_sz, n_emb, ctx_sz, dropout):\n",
    "        super().__init__()\n",
    "        self.keys = NN.Linear(n_emb, head_sz, bias=False)\n",
    "        self.queries = NN.Linear(n_emb, head_sz, bias=False)\n",
    "        self.values = NN.Linear(n_emb, head_sz, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(ctx_sz, ctx_sz)))\n",
    "        self.dropout = NN.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.keys(x) #    (B,T,C)\n",
    "        q = self.queries(x) # (B,T,C)\n",
    "        weights = q @ k.transpose(-2, -1) * C**-0.5 # (B,T,T)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T)\n",
    "        weights = F.softmax(weights, dim=-1) # (B,T,T)\n",
    "        weights = self.dropout(weights)\n",
    "        v = self.values(x) # (B,T,C)\n",
    "        out = weights @ v # (B,T,C)\n",
    "        return out\n",
    "\n",
    "class SelfAttentionMultiHead(NN.Module):\n",
    "    def __init__(self, n_heads, head_sz, n_emb, ctx_sz, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = NN.ModuleList([SelfAttentionHead(head_sz, n_emb, ctx_sz, dropout) for _ in range(n_heads)])\n",
    "        self.proj = NN.Linear(n_emb, n_emb)\n",
    "        self.dropout = NN.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(NN.Module):\n",
    "    def __init__(self, n_emb, multiplier, dropout):\n",
    "        super().__init__()\n",
    "        self.net = NN.Sequential(\n",
    "            NN.Linear(n_emb, n_emb * multiplier),\n",
    "            NN.ReLU(),\n",
    "            NN.Linear(multiplier * n_emb, n_emb),\n",
    "            NN.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(NN.Module):\n",
    "    def __init__(self, n_heads, n_emb, ctx_sz, dropout):\n",
    "        super().__init__()\n",
    "        head_sz = n_emb // n_heads\n",
    "        self.sa = SelfAttentionMultiHead(n_heads, head_sz, n_emb, ctx_sz, dropout)\n",
    "        self.ffwd = FeedForward(n_emb, 4, dropout)\n",
    "        self.ln1 = NN.LayerNorm(n_emb)\n",
    "        self.ln2 = NN.LayerNorm(n_emb)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerLanguageModel(NN.Module):\n",
    "    def __init__(self, n_blocks, n_heads, voc_sz, n_emb, ctx_sz, dropout):\n",
    "        super().__init__()\n",
    "        self.ctx_sz = ctx_sz\n",
    "        self.tok_emb_table = NN.Embedding(voc_sz, n_emb)\n",
    "        self.pos_emb_table = NN.Embedding(ctx_sz, n_emb)\n",
    "        self.blocks = NN.Sequential(*(\n",
    "            ([TransformerBlock(n_heads, n_emb, ctx_sz, dropout)] * n_blocks) +\n",
    "            [NN.LayerNorm(n_emb)]\n",
    "        ))\n",
    "        self.lm_head = NN.Linear(n_emb, voc_sz)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape        \n",
    "        tok_emb = self.tok_emb_table(idx)\n",
    "        pos_emb = self.pos_emb_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            loss = F.cross_entropy(logits, targets.view(B * T))        \n",
    "        return logits, loss\n",
    "    def generate(self, idx, max_new_tok):\n",
    "        for _ in range(max_new_tok):\n",
    "            idx_crop = idx[:, -self.ctx_sz:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            probs = F.softmax(logits[:, -1, :], dim=1)\n",
    "            idx_nxt = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_nxt), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143ea811-f954-4b06-8adb-090fee0a1a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EMB = 384 #32\n",
    "N_HEADS = 6\n",
    "N_BLOCKS = 6\n",
    "EVAL_INT = 1000\n",
    "model = TransformerLanguageModel(N_BLOCKS, N_HEADS, N, N_EMB, CTX_SZ, 0.2).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "@torch.no_grad()\n",
    "def est_loss(model, d):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for splt in ['trn', 'val']:\n",
    "        lossi = torch.zeros(EVAL_INT)\n",
    "        for k in range(EVAL_INT):\n",
    "            bix = torch.randint(d[splt][0].shape[0] - 1, (BATCH_SZ,)).to(device)\n",
    "            _, loss = model(d[splt][0][bix], d[splt][1][bix])\n",
    "            lossi[k] = loss.item()\n",
    "        out[splt] = lossi.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "BATCH_SZ = 64 #32\n",
    "lossi = {'trn':[], 'val':[]}\n",
    "for i in range(15000):\n",
    "    xb, yb = xs_trn[i*BATCH_SZ : (i+1)*BATCH_SZ], ys_trn[i*BATCH_SZ : (i+1)*BATCH_SZ]\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % EVAL_INT == 0:\n",
    "        est = est_loss(model, {'trn': [xs_trn, ys_trn], 'val': [xs_val, ys_val]})\n",
    "        lossi['trn'].append(est['trn'])\n",
    "        lossi['val'].append(est['val'])\n",
    "        print(est['trn'])\n",
    "plt.plot(lossi['trn'])\n",
    "plt.plot(lossi['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92b9556f-0e1b-472a-af7f-eb420a3e1706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t<КАРТИНКА>\n",
      "\tэтотка со своём нет как надо беспоку\n",
      "ВАНЯ:\tДа, кого может фигорея его лишкова?\n",
      "ТИМА:\tУж)\n",
      "ВАНЯ:\tугадай)\n",
      "\t<ЦИТАТА:\tсбер только сгереть?>\n",
      "\theartaple\n",
      "\tв 0\n",
      "\tтебя фраза\n",
      "\tс машах с ними не всё мы как в такоже вторнё наопломанове и стакает атмпачка и андри - та и зависите сошо быть\n",
      "ВАСЯ:\tда из стретьё оставили и понедели светят?\n",
      "\tполезных их признаков, он выехал приду\n",
      "ТИМА:\tэто за хорбыл болтька\n",
      "\tа у тебя у видел - в том, даже обрабоны, видишь проигнал\n",
      "\tпроцент, должно быть будти\n",
      "ВАСЯ:\tменя теперь угрозильно, к чесмодели там дураками\n",
      "ТИМА:\tнадекал\n",
      "ВАНЯ:\tа посмотрим прашлывать)\n",
      "\tс койтоном ндот\n",
      "\tи если есть глопали в деловей мести\n",
      "\tбьюда обучали окно новые леживуют дороге гомна\n",
      "\t<ЦИТАТА:прыганды не занает)>\n",
      "\tнам, там мне помнишь выдобзовывательств Языков ос делали, хмужников себе будет погравитель отвекали:  . А предпроверил, что объявление эту штыку, и могу рассадятые фотографа, уходить сажу не звидимо туто штём добрая\n",
      "\tчто опелил, что это синитер если этим_ени)\n",
      "ВАНЯ:\tну, да эти ши\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.zeros((1,1), dtype=torch.long).to(device), max_new_tok=1000)[0].tolist())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1877d9f-2596-47d9-a3db-c08b40ef8f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "МАША:\tсырный порт:\n",
      "\tНасики стара поздановате нет, испорили сервь>\n",
      "\tВася на нормально) выглядена!!! (\n",
      "СТЁПА:\tнет там вета, как стати\n",
      "\tне не так \"за доровеции!\n",
      "\tЕго Сочьи политься - нибе\n",
      "СТЁПА:\tпервая, ренда влия?\n",
      "ТИМА:\tзато подтавиван момент имень инктески натфильм канате на странели инотянские фотографи в стои кино приез-двет камета идея отклетовая такие и принестива\n",
      "\tE ука z))))\n",
      "СТЁПА:\tтогда да потир отказал\n",
      "\tактёт, даже не радили на должен, которым картом и пусти этих проходимо)\n",
      "ВАНЯ:\tда, информацийна оказали на не ижу)\n",
      "СТЁПА:\tтоже того от сколым их професов\n",
      "ТИМА:\tжуки тется прифрадиван за гоны с судяманный доминками сарианалам и без, непривизование о незаки называть модете выбор политательно силения)\n",
      "\tсмотре не пойде изидеи в жизни, пложу драки не позже только трепирает и и Её\n",
      "СТЁПА:\tда...друга, и балаконец на такой наприпор парашив нос на отку веснове\n",
      "\tина и всех ищут идео и друг она нового\n",
      "\tно датти)\n",
      "\tможет так искать отправь проднепрессов, задошная при должанов ном друзь нора играния\n",
      "СТЁПА:\t\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.tensor([encode(\"МАША:\\tсырный\")], dtype=torch.long).to(device), max_new_tok=1000)[0].tolist())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4751c2-7b5f-4886-ae03-d2e1fe52ec8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
