{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd2454f5-f5bc-4daf-94a6-c0e0ce9166ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as NN\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('data/messages.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02fb0693-d3fd-4976-83ab-84340a105a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bpeasy\n",
    "from bpeasy.tokenizer import BPEasyTokenizer\n",
    "\n",
    "gpt4_regex = r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "vocab = bpeasy.train_bpe(iter([text]), gpt4_regex, 10, 1000)\n",
    "special_tokens = []\n",
    "tokenizer = BPEasyTokenizer(vocab, gpt4_regex, [], fill_to_nearest_multiple_of_eight=True, name=\"skype_msgs_tok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "031e6bd9-75e4-463d-9087-3954544acc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: tokenizer.encode(s)\n",
    "decode = lambda l: tokenizer.decode(l)\n",
    "text_enc = encode(text)\n",
    "LEN = len(text_enc)\n",
    "\n",
    "CTX_SZ = 256 #8\n",
    "xs, ys = [], []\n",
    "for s in range(LEN - CTX_SZ):\n",
    "    xs.append(text_enc[s:s+CTX_SZ])\n",
    "    ys.append(text_enc[s+1:s+CTX_SZ+1])\n",
    "\n",
    "tmp = list(zip(xs, ys))\n",
    "random.shuffle(tmp)\n",
    "xs, ys = zip(*tmp)\n",
    "xs, ys = list(xs), list(ys)\n",
    "\n",
    "n = int(0.9 * LEN)\n",
    "xs_trn, ys_trn = torch.tensor(xs[:n], dtype=torch.int64), torch.tensor(ys[:n], dtype=torch.int64)\n",
    "xs_val, ys_val = torch.tensor(xs[n:], dtype=torch.int64), torch.tensor(ys[n:], dtype=torch.int64)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "xs_trn, ys_trn = xs_trn.to(device), ys_trn.to(device)\n",
    "xs_val, ys_val = xs_val.to(device), ys_val.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5006050d-fe2c-4c60-a08e-e926d46ef602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(NN.Module):\n",
    "    def __init__(self, head_sz, n_emb, ctx_sz, dropout):\n",
    "        super().__init__()\n",
    "        self.keys = NN.Linear(n_emb, head_sz, bias=False)\n",
    "        self.queries = NN.Linear(n_emb, head_sz, bias=False)\n",
    "        self.values = NN.Linear(n_emb, head_sz, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(ctx_sz, ctx_sz)))\n",
    "        self.dropout = NN.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.keys(x) #    (B,T,C)\n",
    "        q = self.queries(x) # (B,T,C)\n",
    "        weights = q @ k.transpose(-2, -1) * C**-0.5 # (B,T,T)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T)\n",
    "        weights = F.softmax(weights, dim=-1) # (B,T,T)\n",
    "        weights = self.dropout(weights)\n",
    "        v = self.values(x) # (B,T,C)\n",
    "        out = weights @ v # (B,T,C)\n",
    "        return out\n",
    "\n",
    "class SelfAttentionMultiHead(NN.Module):\n",
    "    def __init__(self, n_heads, head_sz, n_emb, ctx_sz, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = NN.ModuleList([SelfAttentionHead(head_sz, n_emb, ctx_sz, dropout) for _ in range(n_heads)])\n",
    "        self.proj = NN.Linear(n_emb, n_emb)\n",
    "        self.dropout = NN.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(NN.Module):\n",
    "    def __init__(self, n_emb, multiplier, dropout):\n",
    "        super().__init__()\n",
    "        self.net = NN.Sequential(\n",
    "            NN.Linear(n_emb, n_emb * multiplier),\n",
    "            NN.ReLU(),\n",
    "            NN.Linear(multiplier * n_emb, n_emb),\n",
    "            NN.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(NN.Module):\n",
    "    def __init__(self, n_heads, n_emb, ctx_sz, dropout):\n",
    "        super().__init__()\n",
    "        head_sz = n_emb // n_heads\n",
    "        self.sa = SelfAttentionMultiHead(n_heads, head_sz, n_emb, ctx_sz, dropout)\n",
    "        self.ffwd = FeedForward(n_emb, 4, dropout)\n",
    "        self.ln1 = NN.LayerNorm(n_emb)\n",
    "        self.ln2 = NN.LayerNorm(n_emb)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerLanguageModel(NN.Module):\n",
    "    def __init__(self, n_blocks, n_heads, voc_sz, n_emb, ctx_sz, dropout):\n",
    "        super().__init__()\n",
    "        self.ctx_sz = ctx_sz\n",
    "        self.tok_emb_table = NN.Embedding(voc_sz, n_emb)\n",
    "        self.pos_emb_table = NN.Embedding(ctx_sz, n_emb)\n",
    "        self.blocks = NN.Sequential(*(\n",
    "            ([TransformerBlock(n_heads, n_emb, ctx_sz, dropout)] * n_blocks) +\n",
    "            [NN.LayerNorm(n_emb)]\n",
    "        ))\n",
    "        self.lm_head = NN.Linear(n_emb, voc_sz)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape        \n",
    "        tok_emb = self.tok_emb_table(idx)\n",
    "        pos_emb = self.pos_emb_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            loss = F.cross_entropy(logits, targets.view(B * T))        \n",
    "        return logits, loss\n",
    "    def generate(self, idx, max_new_tok):\n",
    "        for _ in range(max_new_tok):\n",
    "            idx_crop = idx[:, -self.ctx_sz:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            probs = F.softmax(logits[:, -1, :], dim=1)\n",
    "            idx_nxt = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_nxt), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143ea811-f954-4b06-8adb-090fee0a1a08",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 11.00 GiB of which 0 bytes is free. Of the allocated memory 24.74 GiB is allocated by PyTorch, and 64.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15000\u001b[39m):\n\u001b[0;32m     25\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m xs_trn[i\u001b[38;5;241m*\u001b[39mBATCH_SZ : (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mBATCH_SZ], ys_trn[i\u001b[38;5;241m*\u001b[39mBATCH_SZ : (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mBATCH_SZ]\n\u001b[1;32m---> 26\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 70\u001b[0m, in \u001b[0;36mTransformerLanguageModel.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     69\u001b[0m     B, T \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mshape        \n\u001b[1;32m---> 70\u001b[0m     tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtok_emb_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m     72\u001b[0m     x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 11.00 GiB of which 0 bytes is free. Of the allocated memory 24.74 GiB is allocated by PyTorch, and 64.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "N_EMB = 384 #32\n",
    "N_HEADS = 6\n",
    "N_BLOCKS = 6\n",
    "EVAL_INT = 1000\n",
    "model = TransformerLanguageModel(N_BLOCKS, N_HEADS, N, N_EMB, CTX_SZ, 0.2).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "@torch.no_grad()\n",
    "def est_loss(model, d):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for splt in ['trn', 'val']:\n",
    "        lossi = torch.zeros(EVAL_INT)\n",
    "        for k in range(EVAL_INT):\n",
    "            bix = torch.randint(d[splt][0].shape[0] - 1, (BATCH_SZ,)).to(device)\n",
    "            _, loss = model(d[splt][0][bix], d[splt][1][bix])\n",
    "            lossi[k] = loss.item()\n",
    "        out[splt] = lossi.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "BATCH_SZ = 64 #32\n",
    "lossi = {'trn':[], 'val':[]}\n",
    "for i in range(15000):\n",
    "    xb, yb = xs_trn[i*BATCH_SZ : (i+1)*BATCH_SZ], ys_trn[i*BATCH_SZ : (i+1)*BATCH_SZ]\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % EVAL_INT == 0:\n",
    "        est = est_loss(model, {'trn': [xs_trn, ys_trn], 'val': [xs_val, ys_val]})\n",
    "        lossi['trn'].append(est['trn'])\n",
    "        lossi['val'].append(est['val'])\n",
    "        print(est['trn'])\n",
    "plt.plot(lossi['trn'])\n",
    "plt.plot(lossi['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "351a55e1-16bf-4ee5-96d1-d602637ec9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 11.576MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b009b840-667d-4897-91fe-3e10343fb60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2641384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)\n",
    "len(text)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92b9556f-0e1b-472a-af7f-eb420a3e1706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t<КАРТИНКА>\n",
      "\tэтотка со своём нет как надо беспоку\n",
      "ВАНЯ:\tДа, кого может фигорея его лишкова?\n",
      "ТИМА:\tУж)\n",
      "ВАНЯ:\tугадай)\n",
      "\t<ЦИТАТА:\tсбер только сгереть?>\n",
      "\theartaple\n",
      "\tв 0\n",
      "\tтебя фраза\n",
      "\tс машах с ними не всё мы как в такоже вторнё наопломанове и стакает атмпачка и андри - та и зависите сошо быть\n",
      "ВАСЯ:\tда из стретьё оставили и понедели светят?\n",
      "\tполезных их признаков, он выехал приду\n",
      "ТИМА:\tэто за хорбыл болтька\n",
      "\tа у тебя у видел - в том, даже обрабоны, видишь проигнал\n",
      "\tпроцент, должно быть будти\n",
      "ВАСЯ:\tменя теперь угрозильно, к чесмодели там дураками\n",
      "ТИМА:\tнадекал\n",
      "ВАНЯ:\tа посмотрим прашлывать)\n",
      "\tс койтоном ндот\n",
      "\tи если есть глопали в деловей мести\n",
      "\tбьюда обучали окно новые леживуют дороге гомна\n",
      "\t<ЦИТАТА:прыганды не занает)>\n",
      "\tнам, там мне помнишь выдобзовывательств Языков ос делали, хмужников себе будет погравитель отвекали:  . А предпроверил, что объявление эту штыку, и могу рассадятые фотографа, уходить сажу не звидимо туто штём добрая\n",
      "\tчто опелил, что это синитер если этим_ени)\n",
      "ВАНЯ:\tну, да эти ши\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.zeros((1,1), dtype=torch.long).to(device), max_new_tok=1000)[0].tolist())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1877d9f-2596-47d9-a3db-c08b40ef8f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "МАША:\tсырный порт:\n",
      "\tНасики стара поздановате нет, испорили сервь>\n",
      "\tВася на нормально) выглядена!!! (\n",
      "СТЁПА:\tнет там вета, как стати\n",
      "\tне не так \"за доровеции!\n",
      "\tЕго Сочьи политься - нибе\n",
      "СТЁПА:\tпервая, ренда влия?\n",
      "ТИМА:\tзато подтавиван момент имень инктески натфильм канате на странели инотянские фотографи в стои кино приез-двет камета идея отклетовая такие и принестива\n",
      "\tE ука z))))\n",
      "СТЁПА:\tтогда да потир отказал\n",
      "\tактёт, даже не радили на должен, которым картом и пусти этих проходимо)\n",
      "ВАНЯ:\tда, информацийна оказали на не ижу)\n",
      "СТЁПА:\tтоже того от сколым их професов\n",
      "ТИМА:\tжуки тется прифрадиван за гоны с судяманный доминками сарианалам и без, непривизование о незаки называть модете выбор политательно силения)\n",
      "\tсмотре не пойде изидеи в жизни, пложу драки не позже только трепирает и и Её\n",
      "СТЁПА:\tда...друга, и балаконец на такой наприпор парашив нос на отку веснове\n",
      "\tина и всех ищут идео и друг она нового\n",
      "\tно датти)\n",
      "\tможет так искать отправь проднепрессов, задошная при должанов ном друзь нора играния\n",
      "СТЁПА:\t\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.tensor([encode(\"МАША:\\tсырный\")], dtype=torch.long).to(device), max_new_tok=1000)[0].tolist())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4751c2-7b5f-4886-ae03-d2e1fe52ec8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
