{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "26c8e70f-862d-43ea-ad7b-922bfb254af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def make_tok_voc(max_tok, text):\n",
    "    tok_txt = text.encode('utf-8')\n",
    "    tok_txt = list(map(int, tok_txt))\n",
    "    tok_voc = [[t] for t in sorted(list(set(tok_txt)))]\n",
    "    N = len(tok_voc); n = 0\n",
    "    count_sublist = lambda lst, slst: len([1 for idx in range(len(lst)) if lst[idx : idx + len(slst)] == slst])\n",
    "    while N + n < max_tok:\n",
    "        freq_pairs = {}\n",
    "        for i in range(len(tok_txt) - 1):\n",
    "            p = (tok_txt[i], tok_txt[i+1])\n",
    "            if p not in freq_pairs:\n",
    "                freq_pairs[p] = -count_sublist(tok_txt, list(p))\n",
    "        freq_pairs = sorted(freq_pairs.items(), key=lambda kv: kv[1])\n",
    "        if freq_pairs[0][1] < -1:\n",
    "            p = (freq_pairs[0][0][0], freq_pairs[0][0][1])\n",
    "            tok_txt = [-1] + tok_txt  + [-1]\n",
    "            tok_txt = [(256 + n if (t2,t3) == p else t2) for t1, t2, t3 in zip(tok_txt, tok_txt[1:], tok_txt[2:]) if (t1,t2) != p]\n",
    "            tok_voc.append((tok_voc[p[0]-256+N] if p[0] >= 256 else [p[0]]) + (tok_voc[p[1]-256+N] if p[1] >= 256 else [p[1]]))\n",
    "            n += 1\n",
    "            continue\n",
    "        break\n",
    "    return tok_voc\n",
    "\n",
    "def tokenize(text, tok_voc):\n",
    "    res = []; cur = []\n",
    "    for b in text.encode('utf-8'):\n",
    "        nxt = cur + [b]\n",
    "        if (nxt not in tok_voc):\n",
    "            res.append(tok_voc.index(cur))\n",
    "            nxt = [b]\n",
    "        cur = nxt\n",
    "    return res\n",
    "\n",
    "def untokenize(tok_txt, tok_voc):\n",
    "    return bytes(reduce(lambda l1, l2: l1+l2, [tok_voc[i] for i in tok_txt])).decode('utf-8', 'replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "aa7d6907-0811-41b7-a956-1efc60cdb250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32], [33], [40], [41], [44], [45], [46], [48], [51], [63], [66], [73], [83], [84], [85], [87], [95], [97], [98], [99], [100], [101], [102], [103], [104], [105], [107], [108], [109], [110], [111], [112], [114], [115], [116], [117], [118], [119], [120], [121], [122], [128], [131], [132], [133], [135], [137], [140], [142], [143], [146], [147], [148], [152], [153], [156], [157], [158], [159], [164], [168], [169], [170], [174], [179], [180], [181], [186], [188], [189], [226], [239], [240], [101, 32], [240, 159], [226, 128], [105, 110], [115, 32], [97, 110], [116, 104], [240, 159, 133], [240, 159, 135], [97, 114], [239, 189], [226, 128, 140], [226, 128, 140, 240, 159, 135], [101, 114], [111, 114], [116, 32], [105, 110, 103], [115, 116], [97, 110, 100], [32, 116, 104], [44, 32], [97, 109], [32, 116, 104, 101, 32], [111, 117], [85, 110], [85, 110, 105], [85, 110, 105, 99]]\n",
      "[71, 68, 66, 83, 48, 83, 46, 83, 42, 83, 49, 83, 43, 83, 44, 1, 0, 80, 59, 80, 56, 80, 53, 80, 50, 80, 57, 80, 51, 80, 52, 75, 69, 0, 81, 67, 84, 81, 64, 84, 81, 63, 84, 81, 60, 84, 81, 65, 84, 81, 61, 84, 81, 62, 1, 0, 74, 53, 43, 0, 13, 24, 73, 36, 86, 39, 0, 29, 94, 73, 90, 32, 25, 26, 21, 77, 22, 21, 82, 0, 91, 0, 17, 37, 73, 76, 34, 30, 0, 79, 73, 24, 21, 82, 34, 77, 30, 22, 0, 31, 32, 30, 23, 32, 94, 28, 86, 77, 37, 87, 27, 20, 37, 25, 20, 21, 6, 0, 15, 73, 17, 27, 27, 0, 26, 29, 30, 37, 0, 37, 73, 96, 23, 24, 88, 34, 30, 0, 75, 55, 33, 35, 31, 31, 87, 88, 99, 30, 20, 21, 75, 56, 0, 76, 0, 96, 32, 0, 33, 30, 22, 34, 37, 82, 73, 2, 37, 24, 17, 34, 21, 36, 86, 0, 79, 17, 88, 28, 21, 78, 33, 75, 52, 27, 25, 26, 73, 35, 33, 89, 0, 37, 19, 24, 82, 16, 88, 22, 87, 0, 17, 27, 27, 0, 79, 73, 90, 32, 89, 33, 93, 32, 25, 23, 24, 34, 9, 3, 6, 0, 10, 35, 88, 99, 30, 20, 73, 19, 78, 0, 18, 73, 17, 18, 90, 32, 35, 33, 21, 93, 91, 0, 20, 25, 36, 89, 0, 76, 34, 30, 0, 79, 73, 79, 96, 33, 91, 5, 31, 17, 23, 73, 99, 30, 20, 73, 12, 34, 91, 82, 20, 0, 31, 27, 35, 77, 25, 34, 77, 20, 30, 40, 21, 29, 77, 30, 22, 0, 33, 35, 31, 31, 27, 21, 28, 21, 29, 34, 82, 39, 0, 78, 29, 21, 38, 21, 33, 93, 32, 21, 31, 87, 34, 33, 93, 91, 0, 29, 30, 34, 21, 77, 19, 78, 0, 18, 73, 28, 87, 73, 79, 78, 0, 17, 0, 27, 25, 34, 34, 27, 73, 76, 34, 25, 28, 25, 20, 17, 34, 89, 6, 0, 11, 0, 20, 30, 29, 75, 54, 88, 18, 27, 94, 73, 31, 32, 30, 23, 32, 94, 28, 86, 77, 22, 87, 0, 90, 25, 27, 27, 0, 22, 76, 20, 89, 0, 79, 73, 37, 24, 30, 27, 73, 79, 89, 0, 28, 39, 90, 86, 25, 96, 33, 93, 21, 36, 21, 29, 0, 8, 7, 0, 39, 21, 82, 77, 17, 22, 34, 86, 0, 99, 30, 20, 21, 75, 54, 77, 76, 19, 21, 31, 34, 25, 30, 29]\n",
      "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception\n"
     ]
    }
   ],
   "source": [
    "test_text = 'ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.'\n",
    "tok_voc = make_tok_voc(100, test_text)\n",
    "print(tok_voc)\n",
    "print(tokenize(test_text, tok_voc))\n",
    "print(untokenize(tokenize(test_text, tok_voc), tok_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86b162-ca4a-4602-93f3-dffb5d1bdd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
